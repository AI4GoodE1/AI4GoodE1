{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DifferentModelsEvaluation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "79ca8c93c0aa4a1384baea1c1b575e55": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd9f0e7e174f493c80d9454f0a1ca406",
              "IPY_MODEL_0963a3125da64485b2858d119641d8bd",
              "IPY_MODEL_116b0db379fd4eaaa82b57ded7161e45"
            ],
            "layout": "IPY_MODEL_31aefcdf2ba246329f0b603591b2bcc1"
          }
        },
        "dd9f0e7e174f493c80d9454f0a1ca406": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4c69e2d928f481b95db13a0ce435415",
            "placeholder": "​",
            "style": "IPY_MODEL_8fd0d203cadf4ba8929f00d03ef6a342",
            "value": " 81%"
          }
        },
        "0963a3125da64485b2858d119641d8bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6bc153ec3ae34f12939c9874f44034e9",
            "max": 27,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96e2b4aecdf049b8b00f0aaf1ec59f51",
            "value": 22
          }
        },
        "116b0db379fd4eaaa82b57ded7161e45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e88953dd8304389b66ad7bbf639dd77",
            "placeholder": "​",
            "style": "IPY_MODEL_58280a7eb2a24fc5a5f73728fcb4e850",
            "value": " 22/27 [14:34&lt;03:35, 43.14s/it]"
          }
        },
        "31aefcdf2ba246329f0b603591b2bcc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4c69e2d928f481b95db13a0ce435415": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fd0d203cadf4ba8929f00d03ef6a342": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6bc153ec3ae34f12939c9874f44034e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96e2b4aecdf049b8b00f0aaf1ec59f51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e88953dd8304389b66ad7bbf639dd77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58280a7eb2a24fc5a5f73728fcb4e850": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AI4GoodE1/AI4GoodE1/blob/main/DifferentModelsEvaluation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VbfRXHCmOk9y",
        "outputId": "8581a0ba-19ad-4d8f-e226-e8f11be59c4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.19.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.12.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.11.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.5.18.1)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "\n",
        "#imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "# import torch\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "#transformers import\n",
        "from transformers import DistilBertTokenizerFast, DistilBertModel\n",
        "from transformers import TFDistilBertForSequenceClassification, TFTrainer, TFTrainingArguments\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "6oQvwBItJlFF",
        "outputId": "5eeb8c0d-c9c2-42a3-a1f0-bfe1e19d47ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_trained = TFDistilBertForSequenceClassification.from_pretrained(\"/content/drive/Shareddrives/AI4GoodE1 - Scam Detection Project/trained_distilbert_model\")\n",
        "model_20_80 = TFDistilBertForSequenceClassification.from_pretrained(\"/content/drive/Shareddrives/AI4GoodE1 - Scam Detection Project/model_report/20_80\")\n",
        "model_50_50 = TFDistilBertForSequenceClassification.from_pretrained(\"/content/drive/Shareddrives/AI4GoodE1 - Scam Detection Project/model_report/50_50\")\n",
        "model_untrained = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93lRVfpsPEUI",
        "outputId": "03ce6543-628b-4024-a34e-75cb0af73cf6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at /content/drive/Shareddrives/AI4GoodE1 - Scam Detection Project/trained_distilbert_model were not used when initializing TFDistilBertForSequenceClassification: ['dropout_39']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/Shareddrives/AI4GoodE1 - Scam Detection Project/trained_distilbert_model and are newly initialized: ['dropout_99']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some layers from the model checkpoint at /content/drive/Shareddrives/AI4GoodE1 - Scam Detection Project/model_report/20_80 were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/Shareddrives/AI4GoodE1 - Scam Detection Project/model_report/20_80 and are newly initialized: ['dropout_119']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some layers from the model checkpoint at /content/drive/Shareddrives/AI4GoodE1 - Scam Detection Project/model_report/50_50 were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at /content/drive/Shareddrives/AI4GoodE1 - Scam Detection Project/model_report/50_50 and are newly initialized: ['dropout_139']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some layers from the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_projector', 'activation_13', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['dropout_159', 'pre_classifier', 'classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ],
      "metadata": {
        "id": "9147aGzQUvTB"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get data\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/AI4GoodE1/AI4GoodE1/main/fraud_email_preprocessed.csv')\n",
        "X = list(df['Text'])\n",
        "y = list(df['Class'])\n",
        "\n"
      ],
      "metadata": {
        "id": "LJkuFo0ZXryC"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_table = []\n",
        "untrained_table = []"
      ],
      "metadata": {
        "id": "o495Udem9RXY"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model_20_80"
      ],
      "metadata": {
        "id": "otyGI00wWDcO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Trained\n",
        "X_Train, X_Test, y_Train, y_Test = train_test_split(X,y,test_size=0.2,random_state=6,shuffle=True)\n",
        "BATCH_SIZE = 75\n",
        "EPOCHS = len(X_Test) // BATCH_SIZE\n",
        "table = []\n",
        "n,m = 0,BATCH_SIZE\n",
        "trained_pred_array = [] \n",
        "for j in tqdm(range(EPOCHS)):\n",
        "  batch_info = []\n",
        "  batch_num = j+1\n",
        "  batch_info.append(batch_num) # batch number = col 1\n",
        "  for x in range(n,m):\n",
        "    test_tokens = tokenizer(X_Test[x], truncation=True, padding=True, return_tensors=\"tf\")\n",
        "    output = model_20_80(test_tokens)\n",
        "    prediction = np.argmax(output.logits)\n",
        "    trained_pred_array.append(prediction)\n",
        "  true_pos = 0\n",
        "  false_pos = 0\n",
        "  true_neg = 0\n",
        "  false_neg = 0\n",
        "  for i in range(n,m):\n",
        "    if y_Test[i] == trained_pred_array[i]:\n",
        "      if trained_pred_array[i] == 0:\n",
        "        true_neg += 1\n",
        "      else:\n",
        "        true_pos += 1\n",
        "    else:\n",
        "      if trained_pred_array[i] == 0:\n",
        "        false_neg += 1\n",
        "      else:\n",
        "        false_pos += 1\n",
        "  sum_check = true_pos + false_pos + true_neg + false_neg\n",
        "  accuracy_check = ((true_pos + true_neg) / sum_check) * 100\n",
        "  accuracy_check = ((true_pos + true_neg) / sum_check) * 100\n",
        "  recall = (true_pos / (true_pos + false_neg)) * 100\n",
        "  f1_score = (true_pos / (true_pos + 1/2 * ( false_neg + false_pos))) * 100\n",
        "  batch_info.append(accuracy_check) # col 6\n",
        "  batch_info.append(recall) # col 7\n",
        "  batch_info.append(f1_score) # col 8\n",
        "  n,m = n + BATCH_SIZE, m + BATCH_SIZE\n",
        "  table.append(batch_info)\n",
        "  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "79ca8c93c0aa4a1384baea1c1b575e55",
            "dd9f0e7e174f493c80d9454f0a1ca406",
            "0963a3125da64485b2858d119641d8bd",
            "116b0db379fd4eaaa82b57ded7161e45",
            "31aefcdf2ba246329f0b603591b2bcc1",
            "b4c69e2d928f481b95db13a0ce435415",
            "8fd0d203cadf4ba8929f00d03ef6a342",
            "6bc153ec3ae34f12939c9874f44034e9",
            "96e2b4aecdf049b8b00f0aaf1ec59f51",
            "4e88953dd8304389b66ad7bbf639dd77",
            "58280a7eb2a24fc5a5f73728fcb4e850"
          ]
        },
        "id": "QRz-u3fyL4cv",
        "outputId": "32193701-28f2-4606-b723-85dfd7fd5126"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/27 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "79ca8c93c0aa4a1384baea1c1b575e55"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Untrained\n",
        "X_Train, X_Test, y_Train, y_Test = train_test_split(X,y,test_size=0.2,random_state=6,shuffle=True)\n",
        "BATCH_SIZE = 75\n",
        "EPOCHS = len(X_Test) // BATCH_SIZE\n",
        "utable = []\n",
        "n,m = 0,BATCH_SIZE\n",
        "untrained_pred_array = [] \n",
        "for j in tqdm(range(EPOCHS)):\n",
        "  batch_info = []\n",
        "  batch_num = j+1\n",
        "  batch_info.append(batch_num)\n",
        "  for x in range(n,m):\n",
        "    test_tokens = tokenizer(X_Test[x], truncation=True, padding=True, return_tensors=\"tf\")\n",
        "    output = model_untrained(test_tokens)\n",
        "    prediction = np.argmax(output.logits)\n",
        "    untrained_pred_array.append(prediction)\n",
        "  true_pos = 0\n",
        "  false_pos = 0\n",
        "  true_neg = 0\n",
        "  false_neg = 0\n",
        "  for i in range(n,m):\n",
        "    if y_Test[i] == untrained_pred_array[i]:\n",
        "      if untrained_pred_array[i] == 0:\n",
        "        true_neg += 1\n",
        "      else:\n",
        "        true_pos += 1\n",
        "    else:\n",
        "      if untrained_pred_array[i] == 0:\n",
        "        false_neg += 1\n",
        "      else:\n",
        "        false_pos += 1\n",
        "  sum_check = true_pos + false_pos + true_neg + false_neg\n",
        "  accuracy_check = ((true_pos + true_neg) / sum_check) * 100\n",
        "  recall = (true_pos / (true_pos + false_neg)) * 100\n",
        "  f1_score = (true_pos / (true_pos + 1/2 * ( false_neg + false_pos))) * 100\n",
        "  batch_info.append(accuracy_check) # col 6\n",
        "  batch_info.append(recall) # col 7\n",
        "  batch_info.append(f1_score) # col 8\n",
        "  n,m = n + BATCH_SIZE, m + BATCH_SIZE\n",
        "  utable.append(batch_info)"
      ],
      "metadata": {
        "id": "P8fTk4hgNR2E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Untrained\n",
        "df_utable = pd.DataFrame(utable, columns =['BatchNum','Accuracy','Recall','Precision','F1_score']) \n",
        "df_utable = df_utable.drop(['BatchNum'],axis=1)\n",
        "uresults = []\n",
        "df_recallu = df_utable['Recall']\n",
        "mean_recallu = np.mean(df_recallu)\n",
        "df_precisionu = df_utable['Precision']\n",
        "mean_precisionu = np.mean(df_precisionu)\n",
        "df_accuracyu = df_utable['Accuracy']\n",
        "mean_accuracyu = np.mean(df_accuracyu)\n",
        "df_f1scoreu = df_utable['F1_score']\n",
        "mean_f1scoreu = np.mean(df_f1scoreu)\n",
        "uresults.append(mean_recallu)\n",
        "uresults.append(mean_precisionu)\n",
        "uresults.append(mean_f1scoreu)\n",
        "uresults.append(mean_accuracyu)\n",
        "untrained_table.append(uresults)\n",
        "untrained_table"
      ],
      "metadata": {
        "id": "x6WIdCywQ_Vk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Trained\n",
        "df_table = pd.DataFrame(table, columns =['BatchNum','Accuracy','Recall','F1_score']) \n",
        "df_table = df_table.drop(['BatchNum'],axis=1)\n",
        "results = []\n",
        "df_recall = df_table['Recall']\n",
        "mean_recall = np.mean(df_recall)\n",
        "df_accuracy = df_table['Accuracy']\n",
        "mean_accuracy = np.mean(df_accuracy)\n",
        "df_f1score = df_table['F1_score']\n",
        "mean_f1score = np.mean(df_f1score)\n",
        "results.append(mean_recall)\n",
        "results.append(mean_f1score)\n",
        "results.append(mean_accuracy)\n",
        "final_table.append(results)\n",
        "final_table"
      ],
      "metadata": {
        "id": "bk9KeETaZCyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model_70_30\n"
      ],
      "metadata": {
        "id": "uSaOtdZXWJsM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_Train, X_Test, y_Train, y_Test = train_test_split(X,y,test_size=0.3,random_state=0,shuffle=True)\n",
        "BATCH_SIZE = 75\n",
        "EPOCHS = len(X_Test) // BATCH_SIZE\n",
        "table1 = []\n",
        "n,m = 0,BATCH_SIZE\n",
        "trained_pred_array = [] \n",
        "for j in tqdm(range(EPOCHS)):\n",
        "  batch_info = []\n",
        "  batch_num = j+1\n",
        "  batch_info.append(batch_num) # batch number = col 1\n",
        "  for x in range(n,m):\n",
        "    test_tokens = tokenizer(X_Test[x], truncation=True, padding=True, return_tensors=\"tf\")\n",
        "    output = model_trained(test_tokens)\n",
        "    prediction = np.argmax(output.logits)\n",
        "    trained_pred_array.append(prediction)\n",
        "  true_pos = 0\n",
        "  false_pos = 0\n",
        "  true_neg = 0\n",
        "  false_neg = 0\n",
        "  for i in range(n,m):\n",
        "    if y_Test[i] == trained_pred_array[i]:\n",
        "      if trained_pred_array[i] == 0:\n",
        "        true_neg += 1\n",
        "      else:\n",
        "        true_pos += 1\n",
        "    else:\n",
        "      if trained_pred_array[i] == 0:\n",
        "        false_neg += 1\n",
        "      else:\n",
        "        false_pos += 1\n",
        "  sum_check = true_pos + false_pos + true_neg + false_neg\n",
        "  accuracy_check = ((true_pos + true_neg) / sum_check) * 100\n",
        "  recall = (true_pos / (true_pos + false_neg)) * 100\n",
        "  f1_score = (true_pos / (true_pos + 1/2 * ( false_neg + false_pos))) * 100\n",
        "  batch_info.append(accuracy_check) # col 6\n",
        "  batch_info.append(recall) # col 7\n",
        "  batch_info.append(f1_score) # col 8\n",
        "  n,m = n + BATCH_SIZE, m + BATCH_SIZE\n",
        "  table1.append(batch_info)\n",
        "  "
      ],
      "metadata": {
        "id": "_0RsN3fPWPwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Untrained\n",
        "X_Train, X_Test, y_Train, y_Test = train_test_split(X,y,test_size=0.3,random_state=0,shuffle=True)\n",
        "BATCH_SIZE = 75\n",
        "EPOCHS = len(X_Test) // BATCH_SIZE\n",
        "utable = []\n",
        "n,m = 0,BATCH_SIZE\n",
        "trained_pred_array = [] \n",
        "for j in tqdm(range(EPOCHS)):\n",
        "  batch_info = []\n",
        "  batch_num = j+1\n",
        "  batch_info.append(batch_num)\n",
        "  for x in range(n,m):\n",
        "    test_tokens = tokenizer(X_Test[x], truncation=True, padding=True, return_tensors=\"tf\")\n",
        "    output = model_untrained(test_tokens)\n",
        "    prediction = np.argmax(output.logits)\n",
        "    trained_pred_array.append(prediction)\n",
        "  true_pos = 0\n",
        "  false_pos = 0\n",
        "  true_neg = 0\n",
        "  false_neg = 0\n",
        "  for i in range(n,m):\n",
        "    if y_Test[i] == trained_pred_array[i]:\n",
        "      if trained_pred_array[i] == 0:\n",
        "        true_neg += 1\n",
        "      else:\n",
        "        true_pos += 1\n",
        "    else:\n",
        "      if trained_pred_array[i] == 0:\n",
        "        false_neg += 1\n",
        "      else:\n",
        "        false_pos += 1\n",
        "  sum_check = true_pos + false_pos + true_neg + false_neg\n",
        "  accuracy_check = ((true_pos + true_neg) / sum_check) * 100\n",
        "  recall = (true_pos / (true_pos + false_neg)) * 100\n",
        "  f1_score = (true_pos / (true_pos + 1/2 * ( false_neg + false_pos))) * 100\n",
        "  batch_info.append(accuracy_check) # col 6\n",
        "  batch_info.append(recall) # col 7\n",
        "  batch_info.append(f1_score) # col 8\n",
        "  n,m = n + BATCH_SIZE, m + BATCH_SIZE\n",
        "  utable.append(batch_info)"
      ],
      "metadata": {
        "id": "OAxQ2OqlUcwA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Untrained\n",
        "df_utable = pd.DataFrame(utable, columns =['BatchNum','Accuracy','Recall','F1_score']) \n",
        "df_utable = df_utable.drop(['BatchNum'],axis=1)\n",
        "uresults = []\n",
        "df_recallu = df_utable['Recall']\n",
        "mean_recallu = np.mean(df_recallu)\n",
        "df_accuracyu = df_utable['Accuracy']\n",
        "mean_accuracyu = np.mean(df_accuracyu)\n",
        "df_f1scoreu = df_utable['F1_score']\n",
        "mean_f1scoreu = np.mean(df_f1scoreu)\n",
        "uresults.append(mean_recallu)\n",
        "uresults.append(mean_f1scoreu)\n",
        "uresults.append(mean_accuracyu)\n",
        "untrained_table.append(uresults)\n",
        "untrained_table"
      ],
      "metadata": {
        "id": "l50TsmvjUg9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Trained\n",
        "df_table1 = pd.DataFrame(table1, columns =['BatchNum','Accuracy','Recall','F1_score']) \n",
        "df_table1 = df_table1.drop(['BatchNum'],axis=1)\n",
        "results1 = []\n",
        "df_recall1 = df_table1['Recall']\n",
        "mean_recall1 = np.mean(df_recall1)\n",
        "df_accuracy1 = df_table1['Accuracy']\n",
        "mean_accuracy1 = np.mean(df_accuracy1)\n",
        "df_f1score1 = df_table1['F1_score']\n",
        "mean_f1score1 = np.mean(df_f1score1)\n",
        "results1.append(mean_recall1)\n",
        "results1.append(mean_f1score1)\n",
        "results1.append(mean_accuracy1)\n",
        "final_table.append(results1)\n",
        "final_table"
      ],
      "metadata": {
        "id": "TUCpWfksXbN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model_50_50"
      ],
      "metadata": {
        "id": "23JkUsCszyXb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Trained\n",
        "X_Train, X_Test, y_Train, y_Test = train_test_split(X,y,test_size=0.5,random_state=756,shuffle=True)\n",
        "BATCH_SIZE = 125\n",
        "EPOCHS = len(X_Test) // BATCH_SIZE\n",
        "table2 = []\n",
        "n,m = 0,BATCH_SIZE\n",
        "trained_pred_array = [] \n",
        "for j in tqdm(range(EPOCHS)):\n",
        "  batch_info = []\n",
        "  batch_num = j+1\n",
        "  batch_info.append(batch_num) # batch number = col 1\n",
        "  for x in range(n,m):\n",
        "    test_tokens = tokenizer(X_Test[x], truncation=True, padding=True, return_tensors=\"tf\")\n",
        "    output = model_trained(test_tokens)\n",
        "    prediction = np.argmax(output.logits)\n",
        "    trained_pred_array.append(prediction)\n",
        "  true_pos = 0\n",
        "  false_pos = 0\n",
        "  true_neg = 0\n",
        "  false_neg = 0\n",
        "  for i in range(n,m):\n",
        "    if y_Test[i] == trained_pred_array[i]:\n",
        "      if trained_pred_array[i] == 0:\n",
        "        true_neg += 1\n",
        "      else:\n",
        "        true_pos += 1\n",
        "    else:\n",
        "      if trained_pred_array[i] == 0:\n",
        "        false_neg += 1\n",
        "      else:\n",
        "        false_pos += 1\n",
        "  sum_check = true_pos + false_pos + true_neg + false_neg\n",
        "  accuracy_check = ((true_pos + true_neg) / sum_check) * 100\n",
        "  recall = (true_pos / (true_pos + false_neg)) * 100\n",
        "  f1_score = (true_pos / (true_pos + 1/2 * ( false_neg + false_pos))) * 100\n",
        "  batch_info.append(accuracy_check) # col 6\n",
        "  batch_info.append(recall) # col 7\n",
        "  batch_info.append(f1_score) # col 9\n",
        "  n,m = n + BATCH_SIZE, m + BATCH_SIZE\n",
        "  table2.append(batch_info)\n",
        "  "
      ],
      "metadata": {
        "id": "sqgc06-a1xeV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Untrained\n",
        "X_Train, X_Test, y_Train, y_Test = train_test_split(X,y,test_size=0.5,random_state=756,shuffle=True)\n",
        "BATCH_SIZE = 125\n",
        "EPOCHS = len(X_Test) // BATCH_SIZE\n",
        "utable = []\n",
        "n,m = 0,BATCH_SIZE\n",
        "untrained_pred_array = [] \n",
        "for j in tqdm(range(EPOCHS)):\n",
        "  batch_info = []\n",
        "  batch_num = j+1\n",
        "  batch_info.append(batch_num) # batch number = col 1\n",
        "  for x in range(n,m):\n",
        "    test_tokens = tokenizer(X_Test[x], truncation=True, padding=True, return_tensors=\"tf\")\n",
        "    output = model_untrained(test_tokens)\n",
        "    prediction = np.argmax(output.logits)\n",
        "    untrained_pred_array.append(prediction)\n",
        "  true_pos = 0\n",
        "  false_pos = 0\n",
        "  true_neg = 0\n",
        "  false_neg = 0\n",
        "  for i in range(n,m):\n",
        "    if y_Test[i] == untrained_pred_array[i]:\n",
        "      if untrained_pred_array[i] == 0:\n",
        "        true_neg += 1\n",
        "      else:\n",
        "        true_pos += 1\n",
        "    else:\n",
        "      if untrained_pred_array[i] == 0:\n",
        "        false_neg += 1\n",
        "      else:\n",
        "        false_pos += 1\n",
        "  sum_check = true_pos + false_pos + true_neg + false_neg\n",
        "  accuracy_check = ((true_pos + true_neg) / sum_check) * 100\n",
        "  recall = (true_pos / (true_pos + false_neg)) * 100\n",
        "  f1_score = (true_pos / (true_pos + 1/2 * ( false_neg + false_pos))) * 100\n",
        "  batch_info.append(accuracy_check) # col 6\n",
        "  batch_info.append(recall) # col 7\n",
        "  batch_info.append(f1_score) # col 8\n",
        "  n,m = n + BATCH_SIZE, m + BATCH_SIZE\n",
        "  utable.append(batch_info)"
      ],
      "metadata": {
        "id": "Oy9F-AS_DNr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Trained\n",
        "df_table2 = pd.DataFrame(table2, columns =['BatchNum','Accuracy','Recall','F1_score']) \n",
        "df_table2 = df_table2.drop(['BatchNum'],axis=1)\n",
        "results2 = []\n",
        "df_recall2 = df_table2['Recall']\n",
        "mean_recall2 = np.mean(df_recall2)\n",
        "df_accuracy2 = df_table2['Accuracy']\n",
        "mean_accuracy2 = np.mean(df_accuracy2)\n",
        "df_f1score2 = df_table2['F1_score']\n",
        "mean_f1score2 = np.mean(df_f1score2)\n",
        "results2.append(mean_recall2)\n",
        "results2.append(mean_f1score2)\n",
        "results2.append(mean_accuracy2)\n",
        "final_table.append(results2)\n",
        "final_table"
      ],
      "metadata": {
        "id": "PSRUREpE47On"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Untrained\n",
        "df_utable = pd.DataFrame(utable, columns =['BatchNum','Accuracy','Recall','F1_score']) \n",
        "df_utable = df_utable.drop(['BatchNum'],axis=1)\n",
        "uresults = []\n",
        "df_recallu = df_utable['Recall']\n",
        "mean_recallu = np.mean(df_recallu)\n",
        "df_accuracyu = df_utable['Accuracy']\n",
        "mean_accuracyu = np.mean(df_accuracyu)\n",
        "df_f1scoreu = df_utable['F1_score']\n",
        "mean_f1scoreu = np.mean(df_f1scoreu)\n",
        "uresults.append(mean_recallu)\n",
        "uresults.append(mean_f1scoreu)\n",
        "uresults.append(mean_accuracyu)\n",
        "untrained_table.append(uresults)\n",
        "untrained_table"
      ],
      "metadata": {
        "id": "hIziDL0EHTsE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Making the overall table"
      ],
      "metadata": {
        "id": "A3fSAzeBSCk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "final_table"
      ],
      "metadata": {
        "id": "TdHh4XTtSGby"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "untrained_table"
      ],
      "metadata": {
        "id": "NR4PICYLSJtG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_splits = ['20/80','30/70','50/50']\n",
        "df_final_t = pd.DataFrame(final_table, columns = ['Recall','F1 score','Accuracy']) \n",
        "df_untrained_t = pd.DataFrame(untrained_table, columns  = ['Recall','F1 score','Accuracy']) \n",
        "df_untrained_t"
      ],
      "metadata": {
        "id": "0PuUEjVbSJOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_final_t"
      ],
      "metadata": {
        "id": "iLqLaJW3Vvkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_newf = df_final_t.rename(index={0: model_splits[0],1:model_splits[1],2:model_splits[2]})\n",
        "df_newu = df_untrained_t.rename(index={0: model_splits[0],1:model_splits[1],2:model_splits[2]})\n",
        "df_newu"
      ],
      "metadata": {
        "id": "sk6XGKJiXVU3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combine the two tables"
      ],
      "metadata": {
        "id": "Bn0OakS32dnz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}